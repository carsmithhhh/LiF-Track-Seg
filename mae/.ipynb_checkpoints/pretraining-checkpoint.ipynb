{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75dc3dc4-dd93-4eea-ba1f-957c6465355a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNEXT:\\n* Save checkpoints to /sdf/data: yes\\n* Split data into train/val/test\\n* Save 1 image per val epoch\\n* Test method: visualize full patches + reconstructions, all tokens + reconstructions\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "NEXT:\n",
    "* Save checkpoints to /sdf/data: yes\n",
    "* Split data into train/val/test\n",
    "* Save 1 image per val epoch\n",
    "* Test method: visualize full patches + reconstructions, all tokens + reconstructions\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283fa172-b521-49dc-9cf0-a17c15d3a29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from preparation import *\n",
    "from data_utils import *\n",
    "from transformer_things import *\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data import Subset\n",
    "import importlib\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "\n",
    "from pos_embed_model import *\n",
    "import pickle\n",
    "\n",
    "# paths\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3085695c-d987-4f99-99de-54e9c574e8a0",
   "metadata": {},
   "source": [
    "### Loading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebdeec17-9a54-4748-939a-b51d996f6036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f338c625c00>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# Dummy Data\n",
    "# data = np.random.rand(5, 250, 250) # assume each value in the voxel is a fluorescence intensity\n",
    "# data_list = [data] * 50\n",
    "load_patches = np.load('/sdf/data/neutrino/carsmith/all_global_norm_patches.npy') # array of shape (2519, 5, 250, 250)\n",
    "data_list = load_patches.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "800b585d-b60b-44b3-b78a-f321f8a0e033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2519, 5, 250, 250)\n"
     ]
    }
   ],
   "source": [
    "print(load_patches.shape)\n",
    "dataset = CubeDataset(load_patches)\n",
    "\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.05\n",
    "total_size = len(dataset)\n",
    "val_size = int(total_size * val_ratio)\n",
    "test_size = int(total_size * test_ratio)\n",
    "train_size = total_size - val_size - test_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# def custom_collate_fn(batch, mask_percentage=0.6, kernel=12):\n",
    "#     cubes = torch.stack([torch.tensor(cube, dtype=torch.float32) for cube in batch])  # (B, Z, Y, X)\n",
    "#     return cubes # all are (B, Z, Y, X)\n",
    "def custom_collate_fn(batch, mask_percentage=0.6, kernel=12):\n",
    "    cubes = torch.stack([torch.tensor(cube, dtype=torch.float32) for cube in batch])  # (B, Z, Y, X)\n",
    "    cubes = cubes.unsqueeze(1)  # (B, 1, Z, Y, X)\n",
    "    return cubes\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# used to test model capacity by overfitting on a small training set\n",
    "overfit_loader = DataLoader(\n",
    "    Subset(train_dataset, list(range(256))),  # first 10 samples\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=False # we don't give it a full batch\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  # don't shuffle for validation\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  # don't shuffle for test\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab9ccd-4229-45f8-b5a6-010d718f9757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lscratch/carsmith/tmp/ipykernel_1700482/739574938.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cubes = torch.stack([torch.tensor(cube, dtype=torch.float32) for cube in batch])  # (B, Z, Y, X)\n"
     ]
    }
   ],
   "source": [
    "# Testing data loading\n",
    "cubes = next(iter(train_loader))\n",
    "cubes = cubes.to(device)\n",
    "print(len(cubes))\n",
    "print(f\"Cubes shape w/ feature dimension: {cubes.shape}\")\n",
    "\n",
    "test_cubes = next(iter(overfit_loader))\n",
    "print(len(test_cubes))\n",
    "\n",
    "for batch in val_loader:\n",
    "    print(type(batch))\n",
    "    print(batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafe20b9-f8d9-42a4-b00b-eb751d79148e",
   "metadata": {},
   "source": [
    "### Instantiating le Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbee4341-e76f-46c4-8db9-319c2b2351a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = (5, 10, 10)\n",
    "embed_dim = 384\n",
    "px, py, pz = patch_size\n",
    "output_dim = px * py * pz\n",
    "masking_ratio = 0.2 # simple autoencoder test\n",
    "\n",
    "# all the guys\n",
    "patch_model = PatchEmbed3D(patch_size=patch_size, embed_dim=embed_dim).to(device)\n",
    "pos_model = LearnedPositionalEncoder(in_dim=3, embed_dim=embed_dim).to(device)\n",
    "transformer_encoder = VisionTransformer3D(input_dim=embed_dim).to(device)\n",
    "transformer_decoder = Decoder(embed_dim=embed_dim, hidden_dim=1024, num_layers=2, output_dim=output_dim, num_heads=4).to(device)\n",
    "\n",
    "# training params\n",
    "epochs = 400\n",
    "model = Model(patch_model, pos_model, transformer_encoder, transformer_decoder, patch_size, embed_dim, output_dim, device, masking_ratio, patch_size)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b37c2b-5ade-4998-a83f-cf34072f53d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs) \n",
    "\n",
    "# Warmup for first 5 epochs\n",
    "warmup_epochs = 5\n",
    "total_epochs = epochs\n",
    "decay_epochs = total_epochs - warmup_epochs\n",
    "\n",
    "# warmup schedule\n",
    "warmup_scheduler = LinearLR(optimizer, start_factor=1e-2, end_factor=1.0, total_iters=warmup_epochs)\n",
    "\n",
    "# cosine annealing after warmup\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer, T_max=decay_epochs, eta_min=1e-5)  # <- eta_min prevents LR from collapsing\n",
    "\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_epochs])\n",
    "\n",
    "# per epoch stats\n",
    "results = {}\n",
    "results['train_loss'] = []\n",
    "# results['val_loss'] = []\n",
    "results['val_loss'] = []\n",
    "\n",
    "wandb.init(\n",
    "    project=\"mae_pretraining\",\n",
    "    name=\"stratified_training\",\n",
    "    config={\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lr\": lr\n",
    "    }\n",
    ")\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss = pretrain(model, train_loader, optimizer, epoch, epochs, device, patch_size, batch_size)\n",
    "    results['train_loss'].append(train_loss)\n",
    "    val_loss = validate(model, val_loader, epoch, device, patch_size, batch_size=batch_size, save_dir='stratified_val_plots')\n",
    "    results['val_loss'].append(val_loss)\n",
    "    # overfit_loss = overfit_test(model, overfit_loader, epoch, device, patch_size, batch_size, save_dir='smalltoken_mask_plots', plot_num = 1)\n",
    "    # results['overfit_loss'].append(overfit_loss)\n",
    "\n",
    "    wandb.log({\n",
    "        \"pretrain_loss\": train_loss,\n",
    "        # \"overfit_nopos_loss\": overfit_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"learning_rate\": scheduler.get_last_lr()[0],\n",
    "        \"epoch\": epoch,\n",
    "    })\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "        }, f'/sdf/data/neutrino/carsmith/strat_mae_ckpts/checkpoint_{epoch}.pth')\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "np.save('stratified_train_results.npy', results, allow_pickle=True)\n",
    "test_loss = final_test(model, test_loader, device, patch_size, batch_size, save_dir = 'stratified_trained')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90858edb-374b-40b0-b1dc-83aa5115544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "#def final_test(model, test_loader, device='cuda', patch_size=(5, 10, 10), batch_size=64, save_dir='test_imgs':\n",
    "np.save('long_train_results.npy', results, allow_pickle=True)\n",
    "test_loss = final_test(model, test_loader, device, patch_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f3e39a-b82e-4c88-9fd6-ddbdb253dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07677c3-672a-4c48-a778-b6f95293926b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
