{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82037369-f251-45e3-a81d-44737db199d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from preparation import *\n",
    "from data_utils import *\n",
    "from torch.utils.data import DataLoader\n",
    "import importlib\n",
    "\n",
    "# paths\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23e40257-b014-4927-bbfc-06d47447f425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 250, 250)\n"
     ]
    }
   ],
   "source": [
    "# Dummy Data\n",
    "data = np.random.rand(5, 250, 250) # assume each value in the voxel is a fluorescence intensity\n",
    "data_list = [data] * 50\n",
    "\n",
    "dataset = CubeDataset(data_list)\n",
    "\n",
    "sample = dataset[0]\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16e2c2fb-7d18-47c5-bc34-b2bff9fb5031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Cubes shape w/ feature dimension: torch.Size([2, 1, 5, 250, 250])\n"
     ]
    }
   ],
   "source": [
    "def custom_collate_fn(batch, mask_percentage=0.6, kernel=12):\n",
    "    cubes = torch.stack([torch.tensor(cube, dtype=torch.float32) for cube in batch])  # (B, Z, Y, X)\n",
    "    B, Z, Y, X = cubes.shape\n",
    "\n",
    "    masks = torch.ones_like(cubes, dtype=torch.float32)\n",
    "    num_blocks = int(mask_percentage * (Z * Y * X) / (kernel ** 3))\n",
    "\n",
    "    visible_cubes = []\n",
    "\n",
    "    for b in range(B):\n",
    "        for _ in range(num_blocks):\n",
    "            zi = np.random.randint(0, max(Z - kernel, 1))\n",
    "            yi = np.random.randint(0, max(Y - kernel, 1))\n",
    "            xi = np.random.randint(0, max(X - kernel, 1))\n",
    "            masks[b, zi:zi + kernel, yi:yi + kernel, xi:xi + kernel] = 0\n",
    "\n",
    "    masked_cubes = cubes * masks\n",
    "\n",
    "    return cubes, masked_cubes, masks  # all are (B, Z, Y, X)\n",
    "\n",
    "sparse_train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False, # ok because we load data on gpu already ?\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "cubes, masked_cubes, masks = next(iter(sparse_train_loader))\n",
    "cubes = cubes.unsqueeze(dim=1) # shape (B, C=1, Z, Y, X)\n",
    "cubes = cubes.to(device)\n",
    "print(len(cubes))\n",
    "print(f\"Cubes shape w/ feature dimension: {cubes.shape}\")\n",
    "# print(masked_cubes.shape)\n",
    "# print(masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b06af2-6844-4ac1-a9d2-3624c91f3baa",
   "metadata": {},
   "source": [
    "We have a dataset of our original samples. They are (5 x 250 x 250) voxel cubes, or (35 x 106.25 x 106.5 um cubes)\n",
    "\n",
    "Now, we want to embed each of these patches. The PatchEmbed3D layer takes a 3D volumetric image and splits it into non-overlapping 3D patches, then projects each patch into a fixed-length embedding vector. Analogous to how a ViT turns a 2D image into patches. We choose the patch dimension of (5 x 25 x 25) arbitrarily for now (the sample dimension we use is evenly divisilbe by our patch dimensions - important to maintain shape match between patch embeddings and pos embeddings). The patches will be our tokens for our transformer encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4afba4bf-2fc7-4efd-b638-90be70e89011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 128])\n"
     ]
    }
   ],
   "source": [
    "patch_size = (5, 25, 25)\n",
    "patch_embedding_model = PatchEmbed3D(patch_size=patch_size).to(device)\n",
    "patch_embeddings = patch_embedding_model(cubes) # reshape cubes from (B, Z, X, Y) --> (B, N_patches, embed_dim)\n",
    "print(patch_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a615ae1-868a-46b6-96cd-10d5eb8ed11f",
   "metadata": {},
   "source": [
    "Now, we want to calculate a positional encoding for each patch location. We normalize the voxel coordinates within the range [-1, 1] from the centers to encourage stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a6cc2f9-f51f-4513-8dac-f08a4ee095d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch_centers(volume: torch.Tensor, patch_size: tuple) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract normalized patch center positions from a 3D volume.\n",
    "\n",
    "    Args:\n",
    "        volume (Tensor): Input tensor of shape (B, 1, Z, Y, X)\n",
    "        patch_size (tuple): Tuple of (pz, py, px) indicating patch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor: (B, N, 3) tensor of normalized patch centers in [-1, 1]^3\n",
    "    \"\"\"\n",
    "    B, C, Z, Y, X = volume.shape\n",
    "    pz, py, px = patch_size\n",
    "    assert Z % pz == 0 and Y % py == 0 and X % px == 0\n",
    "\n",
    "    # Number of patches in each dim\n",
    "    nz = Z // pz\n",
    "    ny = Y // py\n",
    "    nx = X // px\n",
    "    N = nz * ny * nx  # total number of patches per sample\n",
    "\n",
    "    # Compute the *absolute* (unnormalized) voxel center positions\n",
    "    z_centers = torch.arange(pz//2, Z, step=pz, dtype=torch.float32)\n",
    "    y_centers = torch.arange(py//2, Y, step=py, dtype=torch.float32)\n",
    "    x_centers = torch.arange(px//2, X, step=px, dtype=torch.float32)\n",
    "\n",
    "    zz, yy, xx = torch.meshgrid(z_centers, y_centers, x_centers, indexing=\"ij\")\n",
    "    coords = torch.stack([zz, yy, xx], dim=-1)  # (nz, ny, nx, 3)\n",
    "    coords = coords.view(-1, 3)  # (N, 3)\n",
    "\n",
    "    # Normalize to [-1, 1] using shape\n",
    "    norm_coords = coords.clone()\n",
    "    norm_coords[:, 0] = 2 * (coords[:, 0] / (Z - 1)) - 1\n",
    "    norm_coords[:, 1] = 2 * (coords[:, 1] / (Y - 1)) - 1\n",
    "    norm_coords[:, 2] = 2 * (coords[:, 2] / (X - 1)) - 1\n",
    "\n",
    "    # Repeat for all batch elements\n",
    "    norm_coords = norm_coords.unsqueeze(0).repeat(B, 1, 1)  # (B, N_patches, 3)\n",
    "    return norm_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8556e5dc-ada2-4857-b015-3745c2dfed11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized patch shape: torch.Size([2, 100, 3])\n"
     ]
    }
   ],
   "source": [
    "patch_centers = get_patch_centers(cubes, patch_size)  # (B, N_patches, 3)\n",
    "print(f\"normalized patch shape: {patch_centers.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8774cc96-1986-48ea-8e9b-66047ffca38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos embedding shape: torch.Size([2, 100, 128])\n"
     ]
    }
   ],
   "source": [
    "from pos_embed_model import *\n",
    "\n",
    "pos_encoder = LearnedPositionalEncoder(in_dim=3, embed_dim=128) # using same embed_dim as patch embeddings, currently NOT using intensity values in pos_encoder\n",
    "pos_embeddings = pos_encoder(patch_centers).to(device)\n",
    "print(f\"pos embedding shape: {pos_embeddings.shape}\") # (B, N_patches, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a66511-4deb-49de-80eb-34cd8cfc0eb9",
   "metadata": {},
   "source": [
    "Here is where we mask our embeddings! We define a method to randomly select 60% of our tokens to mask. The masking percentage may be an interesting hyperparameter to tune.\n",
    "\n",
    "Now, we do element-wise add of the patch embeddings and positional embeddings to get our input to the transformer encoder. ViTs and PoLArMAE use elementwise add rather than concatenation, so we follow their convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "870c9198-4126-4778-9c45-b4b65848910a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer input shape: torch.Size([2, 40, 128])\n"
     ]
    }
   ],
   "source": [
    "def random_masking(x, mask_ratio=0.6):\n",
    "    \"\"\"\n",
    "    x: (B, N, D) patch embeddings\n",
    "    Returns:\n",
    "        visible_x: (B, N_vis, D)\n",
    "        mask_indices: (B, N_masked) indices of masked patches\n",
    "        unmask_indices: (B, N_vis) indices of visible patches\n",
    "    \"\"\"\n",
    "    B, N, _ = x.shape\n",
    "    N_vis = int(N * (1 - mask_ratio))\n",
    "\n",
    "    noise = torch.rand(B, N, device=x.device)  # (B, N)\n",
    "    ids_sorted = torch.argsort(noise, dim=1)   # ascending order\n",
    "    ids_keep = ids_sorted[:, :N_vis]\n",
    "    ids_mask = ids_sorted[:, N_vis:]\n",
    "\n",
    "    # Gather visible tokens\n",
    "    batch_idx = torch.arange(B).unsqueeze(-1).to(x.device)  # (B, 1)\n",
    "    x_visible = x[batch_idx, ids_keep]\n",
    "\n",
    "    return x_visible, ids_keep, ids_mask\n",
    "\n",
    "B, N, embed_dim = patch_embeddings.shape\n",
    "patch_embed_vis, ids_keep, ids_mask = random_masking(patch_embeddings, mask_ratio=0.6)\n",
    "patch_embed_vis, ids_keep, ids_mask = patch_embed_vis.to(device), ids_keep.to(device), ids_mask.to(device)\n",
    "pos_embed_vis = pos_embeddings[torch.arange(B).unsqueeze(1), ids_keep].to(device)  # (B, N_visible_patches, D)\n",
    "\n",
    "x = patch_embed_vis + pos_embed_vis\n",
    "print(f\"transformer input shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240480e7-9e55-4e47-ac90-4e6a28dcbfe1",
   "metadata": {},
   "source": [
    "Now we instantiate the transformer. It uses 6 self-attention layers and 4 heads. The transformer does not change the dimension of the input data, it simply applies attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "910e906f-9123-49d5-8133-54aa76e42b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per-patch encoder features shape: torch.Size([2, 40, 128])\n"
     ]
    }
   ],
   "source": [
    "from transformer_things import *\n",
    "transformer_encoder = VisionTransformer3D().to(device)\n",
    "\n",
    "latents = transformer_encoder(x).to(device)\n",
    "print(f\"per-patch encoder features shape: {latents.shape}\") # (B, N_visible_patches, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78381090-7dd6-4d5b-a56f-4bbdf19ed550",
   "metadata": {},
   "source": [
    "Cool cool now we have our encoded latent vectors for the visible tokens. Our next step is to prepare a full token sequence for the transformer decoder. This involves creating a learnable token embedding which is passed in place of all masked tokens, re-adding positional embeddings to all tokens, and placing these in a sequence with the enocded visible tokens.\n",
    "\n",
    "Re-adding positional encodings to ALL tokens is a deviation from what I expected. Explain why this is good later?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9a2632a-714e-4cef-a1ca-34500d6a7882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full decoder input sequence shape: torch.Size([2, 100, 128])\n"
     ]
    }
   ],
   "source": [
    "mask_token = nn.Parameter(torch.zeros(1, 1, 128)).to(device)\n",
    "\n",
    "# Prepare full token sequence for decoder (visible + masked)\n",
    "B, N, D = patch_embeddings.shape\n",
    "x_full = torch.zeros(B, N, D, device=device).to(device)\n",
    "\n",
    "# Fill in visible tokens at their original indices\n",
    "x_full.scatter_(1, ids_keep.unsqueeze(-1).expand(-1, -1, D), patch_embed_vis) # tensor.scatter_ used to place elems from src into tgt at given indices - here x_vis indices\n",
    "\n",
    "# Fill in mask tokens at masked indices\n",
    "x_full.scatter_(1, ids_mask.unsqueeze(-1).expand(-1, -1, D), mask_token.expand(B, ids_mask.size(1), -1))\n",
    "\n",
    "print(f\"full decoder input sequence shape: {x_full.shape}\")\n",
    "\n",
    "# adding positional encodings to all\n",
    "decoder_input = x_full + pos_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a355a33-7d4e-4fcf-a93f-a2be3ae9d0e8",
   "metadata": {},
   "source": [
    "The decoder is trying to predict the raw voxel intensities inside each patch. The decoder is much more lightweight than our encoder - it only has 2 attention layers, whereas the encoder has 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ef336de-aeff-4076-97e3-475124b1fb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: torch.Size([2, 100, 3125])\n",
      "ground truth shape: torch.Size([2, 100, 3125])\n",
      "masked token array shape: torch.Size([2, 60, 3125])\n",
      "tensor(0.6758, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "px, py, pz = patch_size\n",
    "output_dim = px * py * pz\n",
    "\n",
    "decoder = Decoder(embed_dim=128, hidden_dim=256, num_layers=2, output_dim=output_dim, num_heads=4).to(device)\n",
    "recon = decoder(decoder_input)  # (B, N, output_dim) - flattened array of intensity values inside each patch\n",
    "print(f\"Decoder output shape: {recon.shape}\")\n",
    "\n",
    "# ground truth\n",
    "all_patches = extract_voxel_patches(cubes, patch_size)  # (B, N, P)\n",
    "print(f\"ground truth shape: {all_patches.shape}\")\n",
    "\n",
    "# selecting only masked patch ground truths\n",
    "target_masked = all_patches[torch.arange(B).unsqueeze(1), ids_mask]  # (B, N_mask, P)\n",
    "print(f\"masked token array shape: {target_masked.shape}\")\n",
    "\n",
    "# # decoder outputs\n",
    "recon_masked = recon[torch.arange(B).unsqueeze(1), ids_mask]  # (B, N_mask, P)\n",
    "# # Reconstruction loss\n",
    "loss = F.mse_loss(recon_masked, target_masked)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438201f4-9522-4d25-bfb1-244ecb5735d7",
   "metadata": {},
   "source": [
    "Future work: Rather than just trying to reconstruct intensities per voxel in the decoder, we can try to reconstruct x, y, z, 1 --> spatial coordinates of voxel centers and intensities. This may be useful for future downstream tasks like instance detection, and track endpoint localization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b3469b-0dec-4f52-88e4-7df91e141578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
